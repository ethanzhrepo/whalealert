#!/usr/bin/env python3
"""
Analyze Agent - å¤šAgentæ¶ˆæ¯åˆ†æç³»ç»Ÿ
ç›‘å¬NATSæ¶ˆæ¯é˜Ÿåˆ—ï¼Œä½¿ç”¨å¤šä¸ªAI Agentå¯¹Telegramæ¶ˆæ¯è¿›è¡Œåˆ†æå’ŒåŠ å·¥
"""

import asyncio
import json
import logging
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

import yaml
import nats
from pydantic import BaseModel, Field

# LangChain imports
from langchain.schema import BaseMessage, HumanMessage
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# å¯¼å…¥å»é‡æ¨¡å—
from deduplication import get_deduplicator, cleanup_deduplicator, ensure_model_available

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class Config:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_file: str = "config.yml"):
        self.config_file = config_file
        self.config = self._load_config()
        self._setup_logging()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = Path(self.config_file)
        if not config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_config = self.config.get('logging', {})
        level = getattr(logging, log_config.get('level', 'INFO'))
        format_str = log_config.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        
        logging.basicConfig(level=level, format=format_str, force=True)
    
    def get_nats_config(self) -> Dict[str, Any]:
        """è·å–NATSé…ç½®"""
        return self.config.get('nats', {})
    
    def get_llm_config(self) -> Dict[str, Any]:
        """è·å–LLMé…ç½®"""
        return self.config.get('llm', {})
    
    def get_agents_config(self) -> Dict[str, Any]:
        """è·å–Agenté…ç½®"""
        return self.config.get('agents', {})
    
    def get_deduplication_config(self) -> Dict[str, Any]:
        """è·å–å»é‡é…ç½®"""
        return self.config.get('deduplication', {})

class SentimentAnalysisResult(BaseModel):
    """æƒ…ç»ªåˆ†æç»“æœæ¨¡å‹"""
    æƒ…ç»ª: str = Field(description="åˆ©å¤š/åˆ©ç©º/ä¸­æ€§")
    ç†ç”±: str = Field(description="åˆ¤æ–­ç†ç”±")
    æƒ…ç»ªè¯„åˆ†: float = Field(description="æƒ…ç»ªè¯„åˆ†ï¼ŒèŒƒå›´-1.0åˆ°1.0")

class LLMManager:
    """LLMç®¡ç†å™¨ï¼Œæ”¯æŒå¤šç§LLMæä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.provider = config.get('provider', 'ollama')
        self.llm = self._initialize_llm()
    
    def _initialize_llm(self):
        """åˆå§‹åŒ–LLMå®ä¾‹"""
        if self.provider == 'ollama':
            ollama_config = self.config.get('ollama', {})
            return ChatOllama(
                base_url=ollama_config.get('base_url', 'http://localhost:11434'),
                model=ollama_config.get('model', 'llama3.1:8b'),
                temperature=ollama_config.get('temperature', 0.1),
                timeout=ollama_config.get('timeout', 30)
            )
        
        elif self.provider == 'openai':
            openai_config = self.config.get('openai', {})
            return ChatOpenAI(
                api_key=openai_config.get('api_key'),
                model=openai_config.get('model', 'gpt-4o-mini'),
                temperature=openai_config.get('temperature', 0.1),
                max_tokens=openai_config.get('max_tokens', 1000),
                timeout=openai_config.get('timeout', 30)
            )
        
        elif self.provider == 'anthropic':
            anthropic_config = self.config.get('anthropic', {})
            return ChatAnthropic(
                api_key=anthropic_config.get('api_key'),
                model=anthropic_config.get('model', 'claude-3-haiku-20240307'),
                temperature=anthropic_config.get('temperature', 0.1),
                max_tokens=anthropic_config.get('max_tokens', 1000),
                timeout=anthropic_config.get('timeout', 30)
            )
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {self.provider}")
    
    async def generate_response(self, prompt: str) -> str:
        """ç”ŸæˆLLMå“åº”"""
        try:
            messages = [HumanMessage(content=prompt)]
            response = await self.llm.ainvoke(messages)
            return response.content
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise

class BaseAgent(ABC):
    """AgentåŸºç±»"""
    
    def __init__(self, name: str, llm_manager: LLMManager):
        self.name = name
        self.llm_manager = llm_manager
        logger.info(f"åˆå§‹åŒ–Agent: {name}")
    
    @abstractmethod
    async def process(self, message_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†æ¶ˆæ¯çš„æŠ½è±¡æ–¹æ³•"""
        pass
    
    def _extract_raw_text(self, message_data: Dict[str, Any]) -> str:
        """ä»æ¶ˆæ¯æ•°æ®ä¸­æå–raw_text"""
        try:
            # ä¼˜å…ˆä½¿ç”¨extracted_dataä¸­çš„raw_text
            raw_text = message_data.get('data', {}).get('extracted_data', {}).get('raw_text', '')
            if raw_text and raw_text.strip():
                return raw_text.strip()
            
            # é™çº§åˆ°æ™®é€štextå­—æ®µ
            text = message_data.get('data', {}).get('text', '')
            if text and text.strip():
                return text.strip()
            
            # å†é™çº§åˆ°raw_textå­—æ®µï¼ˆç›´æ¥åœ¨dataä¸‹ï¼‰
            raw_text_direct = message_data.get('data', {}).get('raw_text', '')
            if raw_text_direct and raw_text_direct.strip():
                return raw_text_direct.strip()
            
            return ''
        except Exception as e:
            logger.warning(f"æå–raw_textå¤±è´¥: {e}")
            # æœ€åçš„é™çº§æ–¹æ¡ˆ
            return message_data.get('data', {}).get('text', '')

class SentimentAnalysisAgent(BaseAgent):
    """æƒ…ç»ªåˆ†æAgent"""
    
    def __init__(self, llm_manager: LLMManager):
        super().__init__("æƒ…ç»ªåˆ†æAgent", llm_manager)
    
    async def process(self, message_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†æƒ…ç»ªåˆ†æ"""
        start_time = time.time()
        
        raw_text = self._extract_raw_text(message_data)
        
        if not raw_text or len(raw_text.strip()) < 10:
            logger.debug("æ–‡æœ¬å†…å®¹å¤ªçŸ­ï¼Œè·³è¿‡æƒ…ç»ªåˆ†æ")
            return self._create_neutral_result("æ–‡æœ¬å†…å®¹å¤ªçŸ­", start_time)
        
        try:
            # è¾“å‡ºæ­£åœ¨åˆ†æçš„æ–‡æœ¬ä¿¡æ¯
            logger.info(f"ğŸ¤– {self.name} å¼€å§‹åˆ†æ...")
            logger.info(f"ğŸ“Š åˆ†ææ–‡æœ¬é•¿åº¦: {len(raw_text)} å­—ç¬¦")
            if len(raw_text) <= 200:
                logger.info(f"ğŸ“Š åˆ†ææ–‡æœ¬: {raw_text}")
            else:
                logger.info(f"ğŸ“Š åˆ†ææ–‡æœ¬: {raw_text[:200]}... [å·²æˆªæ–­]")
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(raw_text)
            
            # è°ƒç”¨LLM
            logger.info(f"ğŸ”„ è°ƒç”¨ {self.llm_manager.provider} LLM è¿›è¡Œæƒ…ç»ªåˆ†æ...")
            response = await self.llm_manager.generate_response(prompt)
            
            # è§£æå“åº”
            result = self._parse_response(response)
            
            processing_time = int((time.time() - start_time) * 1000)
            
            # è¾“å‡ºåˆ†æç»“æœ
            logger.info(f"âœ… æƒ…ç»ªåˆ†æå®Œæˆ!")
            logger.info(f"ğŸ“ˆ åˆ†æç»“æœ: {result.æƒ…ç»ª} (è¯„åˆ†: {result.æƒ…ç»ªè¯„åˆ†:.2f})")
            logger.info(f"ğŸ’­ åˆ†æç†ç”±: {result.ç†ç”±}")
            logger.info(f"â±ï¸  å¤„ç†è€—æ—¶: {processing_time}ms")
            
            return self._format_result(result, message_data, processing_time)
            
        except Exception as e:
            logger.error(f"âŒ æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
            return self._create_error_result(str(e), message_data, start_time)
    
    def _build_prompt(self, text: str) -> str:
        """æ„å»ºæƒ…ç»ªåˆ†ææç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä½èµ„æ·±åŠ å¯†è´§å¸åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹å¸åœˆæ–°é—»æ‰€ä¼ è¾¾çš„å¸‚åœºæƒ…ç»ªã€‚

æ–°é—»å†…å®¹ï¼š
{text}

è¯·ç›´æ¥è¾“å‡ºå¦‚ä¸‹æ ¼å¼çš„ JSONï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€æ€è€ƒè¿‡ç¨‹æˆ–å…¶ä»–æ–‡å­—ï¼š

{{
  "æƒ…ç»ª": "åˆ©å¤š / åˆ©ç©º / ä¸­æ€§ï¼ˆä¸‰é€‰ä¸€ï¼‰",
  "ç†ç”±": "ä¸€å¥è¯è§£é‡Šä¸ºä»€ä¹ˆä½ åˆ¤æ–­ä¸ºè¿™ç§æƒ…ç»ª",
  "æƒ…ç»ªè¯„åˆ†": æ•°å€¼ï¼ˆèŒƒå›´ä» -1.0 åˆ° 1.0ï¼Œè¶Šæ¥è¿‘ 1 è¡¨ç¤ºè¶Šåˆ©å¤šï¼Œè¶Šæ¥è¿‘ -1 è¡¨ç¤ºè¶Šåˆ©ç©ºï¼‰
}}

åªè¿”å›JSONï¼Œä¸è¦ä»»ä½•å…¶ä»–å†…å®¹ï¼š"""
    
    def _parse_response(self, response: str) -> SentimentAnalysisResult:
        """è§£æLLMå“åº”"""
        try:
            # æ¸…ç†å“åº”å†…å®¹
            response = response.strip()
            
            # ç§»é™¤æ€è€ƒæ ‡ç­¾ï¼ˆå¦‚ <think>...</think>ï¼‰
            import re
            # ç§»é™¤ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹
            response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)
            
            # ç§»é™¤å…¶ä»–å¯èƒ½çš„æ€è€ƒæ ‡ç­¾
            response = re.sub(r'<thinking>.*?</thinking>', '', response, flags=re.DOTALL)
            response = re.sub(r'<thought>.*?</thought>', '', response, flags=re.DOTALL)
            
            # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
            response = response.strip()
            
            # å°è¯•æå–JSONéƒ¨åˆ†
            if response.startswith('```json'):
                response = response[7:]
            if response.endswith('```'):
                response = response[:-3]
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSONå¯¹è±¡
            json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            json_matches = re.findall(json_pattern, response, re.DOTALL)
            
            if json_matches:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„JSONå¯¹è±¡
                response = json_matches[0].strip()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•é€è¡Œè§£æ
                lines = response.split('\n')
                json_lines = []
                in_json = False
                brace_count = 0
                
                for line in lines:
                    line = line.strip()
                    if not in_json and line.startswith('{'):
                        in_json = True
                        json_lines.append(line)
                        brace_count += line.count('{') - line.count('}')
                    elif in_json:
                        json_lines.append(line)
                        brace_count += line.count('{') - line.count('}')
                        if brace_count <= 0:
                            break
                
                if json_lines:
                    response = '\n'.join(json_lines)
            
            # å†æ¬¡æ¸…ç†
            response = response.strip()
            
            logger.debug(f"æ¸…ç†åçš„å“åº”: {response[:200]}...")
            
            # è§£æJSON
            data = json.loads(response)
            
            # éªŒè¯å’Œæ ‡å‡†åŒ–æ•°æ®
            sentiment = data.get('æƒ…ç»ª', 'ä¸­æ€§')
            if sentiment not in ['åˆ©å¤š', 'åˆ©ç©º', 'ä¸­æ€§']:
                sentiment = 'ä¸­æ€§'
            
            reason = data.get('ç†ç”±', 'æ— æ³•ç¡®å®š')
            score = float(data.get('æƒ…ç»ªè¯„åˆ†', 0.0))
            score = max(-1.0, min(1.0, score))  # é™åˆ¶åœ¨[-1, 1]èŒƒå›´å†…
            
            return SentimentAnalysisResult(
                æƒ…ç»ª=sentiment,
                ç†ç”±=reason,
                æƒ…ç»ªè¯„åˆ†=score
            )
            
        except Exception as e:
            logger.warning(f"è§£æLLMå“åº”å¤±è´¥: {e}, åŸå§‹å“åº”: {response[:200]}...")
            return SentimentAnalysisResult(
                æƒ…ç»ª="ä¸­æ€§",
                ç†ç”±="è§£æå¤±è´¥",
                æƒ…ç»ªè¯„åˆ†=0.0
            )
    
    def _format_result(self, result: SentimentAnalysisResult, original_message: Dict[str, Any], processing_time: int) -> Dict[str, Any]:
        """æ ¼å¼åŒ–åˆ†æç»“æœ"""
        return {
            'type': 'analysis.sentiment',
            'timestamp': int(time.time() * 1000),
            'source': 'analyze_agent',
            'sender': 'sentiment_analysis_agent',
            'agent_name': self.name,
            'original_message_id': original_message.get('data', {}).get('message_id'),
            'original_chat_id': original_message.get('data', {}).get('chat_id'),
            'data': {
                'sentiment': result.æƒ…ç»ª,
                'reason': result.ç†ç”±,
                'score': result.æƒ…ç»ªè¯„åˆ†,
                'analysis_time': datetime.now().isoformat(),
                'llm_provider': self.llm_manager.provider,
                'processing_time': processing_time
            }
        }
    
    def _create_neutral_result(self, reason: str, start_time: float) -> Dict[str, Any]:
        """åˆ›å»ºä¸­æ€§ç»“æœ"""
        return {
            'type': 'analysis.sentiment',
            'timestamp': int(time.time() * 1000),
            'source': 'analyze_agent',
            'sender': 'sentiment_analysis_agent',
            'agent_name': self.name,
            'data': {
                'sentiment': 'ä¸­æ€§',
                'reason': reason,
                'score': 0.0,
                'analysis_time': datetime.now().isoformat(),
                'llm_provider': self.llm_manager.provider,
                'processing_time': int((time.time() - start_time) * 1000)
            }
        }
    
    def _create_error_result(self, error: str, original_message: Dict[str, Any], start_time: float) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            'type': 'analysis.error',
            'timestamp': int(time.time() * 1000),
            'source': 'analyze_agent',
            'sender': 'sentiment_analysis_agent',
            'agent_name': self.name,
            'original_message_id': original_message.get('data', {}).get('message_id'),
            'data': {
                'error': error,
                'analysis_time': datetime.now().isoformat(),
                'processing_time': int((time.time() - start_time) * 1000)
            }
        }

class AgentManager:
    """Agentç®¡ç†å™¨"""
    
    def __init__(self, config: Config, llm_manager: LLMManager):
        self.config = config
        self.llm_manager = llm_manager
        self.agents: List[BaseAgent] = []
        self._initialize_agents()
    
    def _initialize_agents(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„Agent"""
        agents_config = self.config.get_agents_config()
        
        # æƒ…ç»ªåˆ†æAgent
        if agents_config.get('sentiment_analysis', {}).get('enabled', False):
            self.agents.append(SentimentAnalysisAgent(self.llm_manager))
        
        # æœªæ¥å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šAgent
        # if agents_config.get('price_analysis', {}).get('enabled', False):
        #     self.agents.append(PriceAnalysisAgent(self.llm_manager))
        
        logger.info(f"å·²åˆå§‹åŒ– {len(self.agents)} ä¸ªAgent")
    
    async def process_message(self, message_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨æ‰€æœ‰Agentå¤„ç†æ¶ˆæ¯ï¼Œè¿”å›æ ¼å¼åŒ–çš„ç»“æœ"""
        start_time = time.time()
        results = []
        successful_count = 0
        failed_count = 0
        
        for agent in self.agents:
            try:
                result = await agent.process(message_data)
                if result:
                    # æ ¼å¼åŒ–Agentç»“æœ
                    agent_result = {
                        'agent_name': agent.name,
                        'agent_type': self._get_agent_type(agent),
                        'result': result.get('data', {}),
                        'processing_time_ms': result.get('data', {}).get('processing_time', 0),
                        'llm_provider': result.get('data', {}).get('llm_provider', ''),
                        'analysis_time': result.get('data', {}).get('analysis_time', datetime.now().isoformat())
                    }
                    results.append(agent_result)
                    
                    if result.get('type') != 'analysis.error':
                        successful_count += 1
                    else:
                        failed_count += 1
                else:
                    failed_count += 1
            except Exception as e:
                logger.error(f"Agent {agent.name} å¤„ç†å¤±è´¥: {e}")
                failed_count += 1
        
        end_time = time.time()
        total_processing_time = int((end_time - start_time) * 1000)
        
        # è®¡ç®—ç»¼åˆæƒ…ç»ªå’Œè¯„åˆ†
        overall_sentiment, overall_score = self._calculate_overall_sentiment(results)
        
        # æ„å»ºæ±‡æ€»ä¿¡æ¯
        summary = {
            'total_agents': len(self.agents),
            'successful_analyses': successful_count,
            'failed_analyses': failed_count,
            'overall_sentiment': overall_sentiment,
            'overall_score': overall_score,
            'processing_start_time': datetime.fromtimestamp(start_time).isoformat() + 'Z',
            'processing_end_time': datetime.fromtimestamp(end_time).isoformat() + 'Z',
            'total_processing_time_ms': total_processing_time
        }
        
        return {
            'analysis_results': results,
            'summary': summary
        }
    
    def _get_agent_type(self, agent: BaseAgent) -> str:
        """è·å–Agentç±»å‹æ ‡è¯†ç¬¦"""
        if isinstance(agent, SentimentAnalysisAgent):
            return 'sentiment_analysis'
        # æœªæ¥å¯ä»¥æ·»åŠ æ›´å¤šAgentç±»å‹
        return 'unknown'
    
    def _calculate_overall_sentiment(self, results: List[Dict[str, Any]]) -> tuple[str, float]:
        """è®¡ç®—ç»¼åˆæƒ…ç»ªå’Œè¯„åˆ†"""
        if not results:
            return 'ä¸­æ€§', 0.0
        
        # ç›®å‰åªæœ‰æƒ…ç»ªåˆ†æAgentï¼Œç›´æ¥ä½¿ç”¨å…¶ç»“æœ
        sentiment_results = [r for r in results if r.get('agent_type') == 'sentiment_analysis']
        
        if not sentiment_results:
            return 'ä¸­æ€§', 0.0
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæƒ…ç»ªåˆ†æç»“æœï¼Œç›´æ¥ä½¿ç”¨
        if len(sentiment_results) == 1:
            result_data = sentiment_results[0].get('result', {})
            return result_data.get('sentiment', 'ä¸­æ€§'), result_data.get('score', 0.0)
        
        # å¦‚æœæœ‰å¤šä¸ªæƒ…ç»ªåˆ†æç»“æœï¼Œè®¡ç®—å¹³å‡å€¼
        total_score = sum(r.get('result', {}).get('score', 0.0) for r in sentiment_results)
        avg_score = total_score / len(sentiment_results)
        
        # æ ¹æ®å¹³å‡è¯„åˆ†ç¡®å®šç»¼åˆæƒ…ç»ª
        if avg_score > 0.3:
            overall_sentiment = 'åˆ©å¤š'
        elif avg_score < -0.3:
            overall_sentiment = 'åˆ©ç©º'
        else:
            overall_sentiment = 'ä¸­æ€§'
        
        return overall_sentiment, avg_score

class AnalyzeAgent:
    """ä¸»åˆ†æç³»ç»Ÿ"""
    
    def __init__(self, config_file: str = "config.yml"):
        self.config = Config(config_file)
        self.llm_manager = LLMManager(self.config.get_llm_config())
        self.agent_manager = AgentManager(self.config, self.llm_manager)
        self.nats_client = None
        self.running = False
        self.deduplicator = None
    
    async def initialize(self):
        """åˆå§‹åŒ–NATSè¿æ¥å’Œå»é‡å™¨"""
        # æ£€æµ‹å’Œåˆå§‹åŒ–å»é‡å™¨
        dedup_config = self.config.get_deduplication_config()
        if dedup_config.get('enabled', False):
            logger.info("æ£€æµ‹æ¶ˆæ¯å»é‡é…ç½®...")
            
            # è·å–æ¨¡å‹åç§°
            model_name = dedup_config.get('model_name', 'BAAI/bge-m3')
            logger.info(f"å»é‡æ¨¡å‹: {model_name}")
            
            # æ£€æŸ¥å¹¶ç¡®ä¿æ¨¡å‹å¯ç”¨
            logger.info("æ£€æŸ¥å»é‡æ¨¡å‹å¯ç”¨æ€§...")
            model_available = await ensure_model_available(model_name)
            
            if not model_available:
                logger.error(f"å»é‡æ¨¡å‹ä¸å¯ç”¨: {model_name}")
                logger.error("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ¨¡å‹è·¯å¾„ï¼Œæˆ–åœ¨é…ç½®ä¸­ç¦ç”¨å»é‡åŠŸèƒ½")
                raise RuntimeError(f"å»é‡æ¨¡å‹ä¸å¯ç”¨: {model_name}")
            
            # åˆå§‹åŒ–å»é‡å™¨ï¼ˆæ­¤æ—¶æ¨¡å‹å·²ç¡®ä¿å¯ç”¨ï¼‰
            logger.info("åˆå§‹åŒ–æ¶ˆæ¯å»é‡å™¨...")
            self.deduplicator = await get_deduplicator(dedup_config)
            logger.info("æ¶ˆæ¯å»é‡å™¨åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.info("æ¶ˆæ¯å»é‡åŠŸèƒ½å·²ç¦ç”¨")
        
        # åˆå§‹åŒ–NATSè¿æ¥
        nats_config = self.config.get_nats_config()
        
        if not nats_config.get('enabled', False):
            raise ValueError("NATSæœªå¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        
        try:
            self.nats_client = await nats.connect(
                servers=nats_config.get('servers', ['nats://localhost:4222'])
            )
            logger.info("NATSè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"NATSè¿æ¥å¤±è´¥: {e}")
            raise
    
    async def start_monitoring(self):
        """å¼€å§‹ç›‘æ§æ¶ˆæ¯"""
        nats_config = self.config.get_nats_config()
        subjects = nats_config.get('subject', [])
        
        if not subjects:
            raise ValueError("æœªé…ç½®ç›‘æ§çš„subject")
        
        logger.info(f"å¼€å§‹ç›‘æ§NATS subjects: {subjects}")
        
        # è®¢é˜…æ‰€æœ‰é…ç½®çš„subject
        for subject in subjects:
            await self.nats_client.subscribe(subject, cb=self._message_handler)
            logger.info(f"å·²è®¢é˜…subject: {subject}")
        
        self.running = True
        logger.info("æ¶ˆæ¯ç›‘æ§å·²å¯åŠ¨ï¼Œç­‰å¾…æ¶ˆæ¯...")
        logger.info("å¦‚æœé•¿æ—¶é—´æ²¡æœ‰æ”¶åˆ°æ¶ˆæ¯ï¼Œè¯·æ£€æŸ¥:")
        logger.info("1. telegramstreamæ˜¯å¦æ­£åœ¨è¿è¡Œ")
        logger.info("2. telegramstreamçš„NATS subjecté…ç½®æ˜¯å¦æ­£ç¡®")
        logger.info("3. NATSæœåŠ¡å™¨è¿æ¥æ˜¯å¦æ­£å¸¸")
        
        try:
            # ä¿æŒè¿è¡Œ
            while self.running:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·")
        finally:
            self.running = False
            if self.nats_client:
                await self.nats_client.close()
            # æ¸…ç†å»é‡å™¨
            if self.deduplicator:
                await cleanup_deduplicator()
    
    async def _message_handler(self, msg):
        """å¤„ç†æ¥æ”¶åˆ°çš„æ¶ˆæ¯"""
        try:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            logger.info(f"æ”¶åˆ°NATSæ¶ˆæ¯ [subject: {msg.subject}], å¤§å°: {len(msg.data)} bytes")
            
            # è§£ææ¶ˆæ¯
            message_data = json.loads(msg.data.decode())
            
            logger.info(f"è§£ææ¶ˆæ¯æˆåŠŸ: type={message_data.get('type')}, source={message_data.get('source')}")
            
            # å¤„ç†telegramå’Œtwitteræ¶ˆæ¯
            source = message_data.get('source')
            if source not in ['telegram', 'twitter']:
                logger.info(f"è·³è¿‡ä¸æ”¯æŒçš„æ¶ˆæ¯æº: source={source}")
                return
            
            # æå–å¹¶è¾“å‡ºåŸæ–‡å’Œæ¥æºä¿¡æ¯
            data = message_data.get('data', {})
            text = data.get('text', '') or data.get('raw_text', '')
            username = data.get('username', '') or data.get('user_id', '')
            chat_title = data.get('chat_title', '') if source == 'telegram' else ''
            list_url = data.get('list_url', '') if source == 'twitter' else ''
            message_id = data.get('message_id', '')
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºæ‰€æœ‰å¯èƒ½çš„æ–‡æœ¬å­—æ®µ
            logger.debug(f"ğŸ” è°ƒè¯•æ–‡æœ¬å­—æ®µ:")
            logger.debug(f"   data.text: '{data.get('text', '')}'")
            logger.debug(f"   data.raw_text: '{data.get('raw_text', '')}'")
            logger.debug(f"   data.extracted_data.raw_text: '{data.get('extracted_data', {}).get('raw_text', '')}'")
            
            # è¾“å‡ºæ¶ˆæ¯æ¥æºå’ŒåŸæ–‡ä¿¡æ¯
            logger.info("=" * 80)
            logger.info(f"ğŸ“¨ æ¶ˆæ¯æ¥æº: {source.upper()}")
            logger.info(f"ğŸ†” æ¶ˆæ¯ID: {message_id}")
            if source == 'telegram':
                logger.info(f"ğŸ‘¤ ç”¨æˆ·: {username}")
                if chat_title:
                    logger.info(f"ğŸ’¬ ç¾¤ç»„: {chat_title}")
            elif source == 'twitter':
                logger.info(f"ğŸ‘¤ ç”¨æˆ·: @{username}")
                if list_url:
                    logger.info(f"ğŸ“‹ åˆ—è¡¨: {list_url}")
            
            # è¾“å‡ºåŸæ–‡å†…å®¹
            if text:
                logger.info(f"ğŸ“ åŸæ–‡å†…å®¹:")
                # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œæˆªæ–­æ˜¾ç¤º
                if len(text) > 500:
                    logger.info(f"   {text[:500]}...")
                    logger.info(f"   [æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦ï¼Œå·²æˆªæ–­æ˜¾ç¤º]")
                else:
                    logger.info(f"   {text}")
            else:
                logger.info("ğŸ“ åŸæ–‡å†…å®¹: [æ— æ–‡æœ¬å†…å®¹]")
            
            logger.info("=" * 80)
            
            logger.info(f"å¼€å§‹å¤„ç†{source}æ¶ˆæ¯: {message_data.get('type')}")
            
            # æ¶ˆæ¯å»é‡æ£€æŸ¥
            if self.deduplicator:
                is_duplicate, similar_record, similarity_score = await self.deduplicator.check_duplicate(message_data)
                
                if is_duplicate:
                    logger.info(f"æ£€æµ‹åˆ°é‡å¤æ¶ˆæ¯ï¼Œè·³è¿‡å¤„ç†: ç›¸ä¼¼åº¦={similarity_score:.3f}")
                    
                    # è®°å½•å»é‡ç»Ÿè®¡ä¿¡æ¯
                    stats = self.deduplicator.get_stats()
                    logger.info(f"å»é‡ç»Ÿè®¡: æ€»æ¶ˆæ¯={stats['total_messages']}, é‡å¤={stats['duplicates_found']}, ç¼“å­˜å¤§å°={stats['cache_size']}")
                    
                    # å‘é€å»é‡é€šçŸ¥
                    await self._send_duplicate_notification(message_data, similar_record, similarity_score)
                    return
                
                # æ·»åŠ æ¶ˆæ¯åˆ°å»é‡ç¼“å­˜
                await self.deduplicator.add_message(message_data)
            
            # ä½¿ç”¨Agentå¤„ç†æ¶ˆæ¯
            analysis_result = await self.agent_manager.process_message(message_data)
            
            logger.info(f"Agentå¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(analysis_result['analysis_results'])} ä¸ªç»“æœ")
            
            # è¾“å‡ºåˆ†æç»“æœ
            for result in analysis_result['analysis_results']:
                result_json = json.dumps(result, ensure_ascii=False, separators=(',', ':'))
                print(f"[{datetime.now().isoformat()}] {result_json}")
            
            # å‘é€é€šçŸ¥æ¶ˆæ¯åˆ° messages.notification subject
            await self._send_notification(message_data, analysis_result)
                
        except Exception as e:
            logger.error(f"å¤„ç†æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
    
    async def _send_duplicate_notification(self, message_data: Dict[str, Any], similar_record, similarity_score: float):
        """å‘é€é‡å¤æ¶ˆæ¯é€šçŸ¥"""
        try:
            nats_config = self.config.get_nats_config()
            notification_subject = nats_config.get('notification_subject', 'messages.notification')
            
            # ç¡®ä¿similarity_scoreæ˜¯PythonåŸç”Ÿfloatç±»å‹ï¼Œé¿å…numpyç±»å‹åºåˆ—åŒ–é—®é¢˜
            similarity_score_float = float(similarity_score) if similarity_score is not None else 0.0
            
            # æ„å»ºé‡å¤æ¶ˆæ¯é€šçŸ¥
            notification_message = {
                'type': 'messages.duplicate',
                'timestamp': int(time.time() * 1000),
                'source': 'analyze_agent',
                'sender': 'deduplication_agent',
                'data': {
                    'message': message_data,
                    'duplicate_info': {
                        'is_duplicate': True,
                        'similarity_score': similarity_score_float,
                        'original_message_id': similar_record.message_id if similar_record else None,
                        'original_timestamp': float(similar_record.timestamp) if similar_record else None,
                        'detection_time': datetime.now().isoformat()
                    },
                    'stats': self._sanitize_stats(self.deduplicator.get_stats() if self.deduplicator else {})
                }
            }
            
            # å‘é€åˆ°NATS
            notification_json = json.dumps(notification_message, ensure_ascii=False, separators=(',', ':'))
            await self.nats_client.publish(notification_subject, notification_json.encode())
            
            logger.debug(f"é‡å¤æ¶ˆæ¯é€šçŸ¥å·²å‘é€åˆ° {notification_subject}")
            
        except Exception as e:
            logger.error(f"å‘é€é‡å¤æ¶ˆæ¯é€šçŸ¥å¤±è´¥: {e}")
    
    def _sanitize_stats(self, stats: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…ç†ç»Ÿè®¡æ•°æ®ä¸­çš„numpyç±»å‹ï¼Œç¡®ä¿JSONåºåˆ—åŒ–å…¼å®¹"""
        import numpy as np
        
        sanitized = {}
        for key, value in stats.items():
            if isinstance(value, (np.integer, np.floating)):
                sanitized[key] = value.item()  # è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
            elif isinstance(value, np.ndarray):
                sanitized[key] = value.tolist()  # è½¬æ¢ä¸ºPythonåˆ—è¡¨
            elif isinstance(value, dict):
                sanitized[key] = self._sanitize_stats(value)  # é€’å½’å¤„ç†åµŒå¥—å­—å…¸
            elif isinstance(value, list):
                sanitized[key] = [self._sanitize_value(item) for item in value]
            else:
                sanitized[key] = value
        return sanitized
    
    def _sanitize_value(self, value):
        """æ¸…ç†å•ä¸ªå€¼ä¸­çš„numpyç±»å‹"""
        import numpy as np
        
        if isinstance(value, (np.integer, np.floating)):
            return value.item()
        elif isinstance(value, np.ndarray):
            return value.tolist()
        elif isinstance(value, dict):
            return self._sanitize_stats(value)
        elif isinstance(value, list):
            return [self._sanitize_value(item) for item in value]
        else:
            return value
    
    async def _send_notification(self, original_message: Dict[str, Any], analysis_result: Dict[str, Any]):
        """å‘é€é€šçŸ¥æ¶ˆæ¯åˆ° messages.notification subject"""
        try:
            nats_config = self.config.get_nats_config()
            notification_subject = nats_config.get('notification_subject', 'messages.notification')
            
            # æ·»åŠ å»é‡ç»Ÿè®¡ä¿¡æ¯åˆ°é€šçŸ¥ä¸­
            notification_data = {
                'original_message': original_message,
                'analysis_results': analysis_result['analysis_results'],
                'summary': analysis_result['summary']
            }
            
            # å¦‚æœå¯ç”¨äº†å»é‡ï¼Œæ·»åŠ å»é‡ç»Ÿè®¡ä¿¡æ¯
            if self.deduplicator:
                notification_data['deduplication_stats'] = self._sanitize_stats(self.deduplicator.get_stats())
            
            # æ„å»ºé€šçŸ¥æ¶ˆæ¯
            notification_message = {
                'type': 'messages.notification',
                'timestamp': int(time.time() * 1000),
                'source': 'analyze_agent',
                'sender': 'analyze_agent',
                'data': notification_data
            }
            
            # å‘é€åˆ°NATS
            notification_json = json.dumps(notification_message, ensure_ascii=False, separators=(',', ':'))
            await self.nats_client.publish(notification_subject, notification_json.encode())
            
            logger.info(f"é€šçŸ¥æ¶ˆæ¯å·²å‘é€åˆ° {notification_subject}")
            logger.debug(f"é€šçŸ¥æ¶ˆæ¯å†…å®¹: {notification_json[:200]}...")
            
        except Exception as e:
            logger.error(f"å‘é€é€šçŸ¥æ¶ˆæ¯å¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºåˆ†æç³»ç»Ÿ
        analyzer = AnalyzeAgent()
        
        # åˆå§‹åŒ–
        await analyzer.initialize()
        
        # å¼€å§‹ç›‘æ§
        await analyzer.start_monitoring()
        
    except Exception as e:
        logger.error(f"ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == '__main__':
    import sys
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        sys.exit(1)
